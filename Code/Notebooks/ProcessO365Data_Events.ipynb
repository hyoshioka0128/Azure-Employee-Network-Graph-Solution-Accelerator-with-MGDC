{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "data_lake_account_name = '' # Synapse Workspace ADLS\n",
        "file_system_name = ''\n",
        "subfolder_name = ''\n",
        "folder_name = 'events'\n",
        "user_group_name = ''\n",
        "initialLoad = 'false'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "base_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "def get_location_flag(Location_DisplayName): \n",
        "    location_flag = 'InPerson'\n",
        "    if 'teams' in Location_DisplayName or 'zoom' in Location_DisplayName \\\n",
        "    or 'webex' in Location_DisplayName or 'loopup' in Location_DisplayName \\\n",
        "    or 'dial' in Location_DisplayName or 'conference' in Location_DisplayName  \\\n",
        "    or '+1' in Location_DisplayName or 'meeting' in Location_DisplayName  \\\n",
        "    or 'cell' in Location_DisplayName or '1-8' in Location_DisplayName  \\\n",
        "    or 'code' in Location_DisplayName or '888' in Location_DisplayName  \\\n",
        "    or '800' in Location_DisplayName or 'call' in Location_DisplayName  \\\n",
        "    or '#' in Location_DisplayName or 'google meet' in Location_DisplayName :\n",
        "        location_flag = 'Remote'\n",
        "    return location_flag\n",
        "    \n",
        "def load_events_json_file(eventsPath):\n",
        "    events_df= spark.read.load(eventsPath, format='json')\n",
        "\n",
        "    location_flag_udf = udf(lambda Location_DisplayName: get_location_flag(Location_DisplayName), returnType=StringType())\n",
        "\n",
        "    df = events_df.select('*',size('Attendees').alias('Attendees_cnt')) \\\n",
        "                .select(explode(col(\"Attendees\")).alias(\"Attendees\"),\"Id\",\"puser\",\"ICalUid\",\"Subject\",\"Recurrence\",\"IsCancelled\",\"Start\",\"End\",\"CreatedDateTime\",\"Organizer\",\"Attendees_cnt\",\"Location\") \\\n",
        "                .select([\"Id\",\"puser\",\"ICalUid\",\"Subject\",\"Attendees_cnt\",\n",
        "                            when(col(\"Recurrence\").isNull(),False).otherwise(True).alias(\"Recurrence\"),\n",
        "                            \"IsCancelled\",\n",
        "                            col(\"Start.DateTime\").alias(\"Start\"),col(\"End.DateTime\").alias(\"End\"),\n",
        "                            col(\"CreatedDateTime\").alias(\"CreatedDateTime\"),\n",
        "                            col(\"Organizer.EmailAddress.Address\").alias(\"Organizer\"),\n",
        "                            col(\"Attendees.EmailAddress.Address\").alias(\"Attendee\"),\n",
        "                            col(\"Attendees.Status.Response\").alias(\"Attendee_Response\"),\n",
        "                            col(\"Attendees.Type\").alias(\"Attendee_Type\"),\n",
        "                            col(\"Location.Address.Type\").alias(\"Location_Address_Type\"),\n",
        "                            col(\"Location.DisplayName\").alias(\"Location_DisplayName\")]) \\\n",
        "                .withColumn('location_flag', location_flag_udf(lower(col(\"Location_DisplayName\")))) \\\n",
        "                .withColumn(\"LoadDateRange\", lit(subfolder_name)) \\\n",
        "                .withColumn(\"UserGroup\", lit(user_group_name))\n",
        "    try:\n",
        "        df = df.withColumn('Start', to_timestamp('Start')) \\\n",
        "                .withColumn('Start_Date', to_date('Start')) \\\n",
        "                .withColumn('End', to_timestamp('End')) \\\n",
        "                .withColumn('End_Date', to_date('End')) \\\n",
        "                .withColumn('Created_Date', to_date('CreatedDateTime')) \\\n",
        "                .withColumn('Organizer', lower(col('Organizer'))) \\\n",
        "                .withColumn('Attendee', lower(col('Attendee'))) \\\n",
        "                .withColumn('Organizer_Domain', reverse(split(lower(col('Organizer')),'@'))[0]) \\\n",
        "                .withColumn('Attendee_Domain', reverse(split(lower(col('Attendee')),'@'))[0]) \\\n",
        "                .select('Id','puser','ICalUid','Subject','Recurrence','IsCancelled', \\\n",
        "                        'Start','Start_Date','End','CreatedDateTime','Created_Date','Organizer','Attendee','Attendee_Response', \\\n",
        "                        'Attendee_Type','Organizer_Domain','Attendee_Domain','Attendees_cnt','location_flag','LoadDateRange')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    processed_path = base_path +\"o365data_processed/\" + folder_name + \"/\" + user_group_name + \"/\" + subfolder_name\n",
        "    df.write.format(\"parquet\").mode(\"append\").option(\"overwriteSchema\", \"true\").save(processed_path)\n",
        "  \n",
        "def get_event_subfolder_files(folder):\n",
        "    children = mssparkutils.fs.ls(folder)\n",
        "    for child in children:\n",
        "        if child.name == 'metadata':\n",
        "            continue\n",
        "        if child.isDir:\n",
        "            get_event_subfolder_files(child.path)\n",
        "        else:\n",
        "            load_events_json_file(child.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "eventsPath = base_path +\"o365data/\" + folder_name + \"/\" + user_group_name + \"/\" + subfolder_name\n",
        "\n",
        "\n",
        "get_event_subfolder_files(eventsPath) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "processed_path = base_path +\"o365data_processed/\" + folder_name + \"/\" + user_group_name + \"/\" + subfolder_name\n",
        "df_events = spark.read.format(\"parquet\").load(processed_path,header=True)\n",
        "df_events = df_events.select('Id','Organizer','Attendee','Start_Date','CreatedDateTime','Created_Date','Organizer_Domain','Attendee_Domain','Attendee_Response')\n",
        "\n",
        "df_events = df_events.withColumn('IsReversed_Row',lit(0))\n",
        "\n",
        "df_events_copy = df_events.select('Id','Organizer','Attendee','Start_Date','CreatedDateTime','Created_Date','Organizer_Domain','Attendee_Domain','Attendee_Response') \\\n",
        "                              .withColumn('Organizer',col('Attendee')) \\\n",
        "                              .withColumn('Attendee',col('Organizer')) \\\n",
        "                              .withColumn('Organizer_Domain',col('Attendee_Domain')) \\\n",
        "                              .withColumn('Attendee_Domain',col('Organizer_Domain')) \\\n",
        "                              .withColumn('IsReversed_Row',lit(1))\n",
        "df_events = df_events.union(df_events_copy)\n",
        "\n",
        "df_events.write.mode(\"append\").saveAsTable(\"eventsdata\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
