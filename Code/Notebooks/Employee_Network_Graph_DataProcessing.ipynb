{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, split,regexp_replace,reverse,initcap, lit, to_date, lower"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_lake_account_name = ''\r\n",
        "file_system_name = ''\r\n",
        "# name of the secret in key vault for your text analytics key\r\n",
        "text_analytics_Key = ''\r\n",
        "# name of the secret in key vault for your text analytics endpoint\r\n",
        "text_analytics_endpoint = ''\r\n",
        "# region your text analytics resource is in \r\n",
        "text_analytics_region = ''  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/enroncsvdata/enron_05_17_2015_with_labels_v2.csv\"\r\n",
        "df = spark.read.load(file_path, format='csv', header=True,quote='\"', delimiter=',',escape='\"',multiline=True)\r\n",
        "\r\n",
        "df = df.select('Message-ID', 'Date','From','To','Subject','X-From','X-To','X-Folder','content','user')\r\n",
        "\r\n",
        "df = df.withColumn('X-Folder', reverse(split(col('X-Folder'), \"\\'\")).getItem(0))\r\n",
        "\r\n",
        "df = df.withColumn('From', split(col('From'), \"'\").getItem(1))\r\n",
        "df = df.withColumn('To', split(col('To'), \"'\").getItem(1))\r\n",
        "\r\n",
        "\r\n",
        "df = df.withColumnRenamed('From','source') \\\r\n",
        "       .withColumnRenamed('To','target')\r\n",
        "\r\n",
        "df = df.withColumn('sourcename', initcap(regexp_replace(split(col('source'), \"@\").getItem(0),'\\.',' ')))\r\n",
        "df = df.withColumn('targetname', initcap(regexp_replace(split(col('target'), \"@\").getItem(0),'\\.',' ')))\r\n",
        "\r\n",
        "df = df.withColumn('sourcedomain', split(col('source'), \"@\").getItem(1))\r\n",
        "df = df.withColumn('targetdomain', split(col('target'), \"@\").getItem(1))\r\n",
        "\r\n",
        "\r\n",
        "df = df.limit(10000)\r\n",
        "\r\n",
        "df = df.filter(df[\"source\"].isNotNull())\r\n",
        "df = df.filter(df[\"target\"].isNotNull())\r\n",
        "\r\n",
        "import uuid\r\n",
        "\r\n",
        "# map column names \r\n",
        "df = df.withColumn('Id', col('Message-ID')) \\\r\n",
        "       .withColumn('puser', lit(str(uuid.uuid4()))) \\\r\n",
        "       .withColumn('ParentFolderId', lit(str(uuid.uuid4()))) \\\r\n",
        "       .withColumn('ConversationId', lit(str(uuid.uuid4()))) \\\r\n",
        "       .withColumn('Subject', col('Subject')) \\\r\n",
        "       .withColumn('UniqueBody', col('content')) \\\r\n",
        "       .withColumn('CreatedDateTime', col('Date')) \\\r\n",
        "       .withColumn('LastModifiedDateTime', col('Date')) \\\r\n",
        "       .withColumn('SentDateTime', col('Date')) \\\r\n",
        "       .withColumn('SentDate', to_date('Date')) \\\r\n",
        "       .withColumn('Sender', lower('source')) \\\r\n",
        "       .withColumn('Recipient', lower('target')) \\\r\n",
        "       .withColumn('RType', lit('To')) \\\r\n",
        "       .withColumn('Sender_Domain', col('sourcedomain')) \\\r\n",
        "       .withColumn('Recipient_Domain', col('targetdomain')) \\\r\n",
        "       .withColumn('ToRecipients_cnt', lit(1)) \\\r\n",
        "       .withColumn('CcRecipients_cnt', lit(0)) \\\r\n",
        "       .withColumn('BccRecipients_cnt', lit(0)) \\\r\n",
        "       .withColumn('Recipients_cnt', (col('ToRecipients_cnt') + col('CcRecipients_cnt') + col('BccRecipients_cnt'))) \\\r\n",
        "       .withColumn('LoadFolderPath', lit('test_path')) \\\r\n",
        "       .select('Id','puser','ParentFolderId','ConversationId','Subject','UniqueBody','CreatedDateTime','LastModifiedDateTime','SentDateTime',\r\n",
        "       'SentDate','Sender','Recipient','RType','Sender_Domain','Recipient_Domain','ToRecipients_cnt','CcRecipients_cnt','BccRecipients_cnt','Recipients_cnt','LoadFolderPath')\r\n",
        "\r\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"messages_all_data\")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_events = spark.sql('''select Id, puser, Id as ICalUid, Subject, False as Recurrence, False as IsCancelled,\r\n",
        "             SentDateTime as Start, SentDate as Start_Date, SentDateTime as End, Sender as Organizer, Recipient as Attendee, \r\n",
        "             None as Attendee_Response, 'Required' as Attendee_Type, Sender_Domain as Organizer_Domain, Recipient_Domain as Attendee_Domain,\r\n",
        "             2 as Attendees_cnt, 'Remote' as location_flag'''\r\n",
        "\r\n",
        "df_events.write.mode(\"overwrite\").saveAsTable(\"events_all_data\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_messages = spark.sql('''select * from messages_all_data where Sender in \r\n",
        "(\r\n",
        "    select Sender from messages_all_data\r\n",
        "    where Sender_domain = 'enron.com'\r\n",
        "    group by Sender\r\n",
        "    order by count(distinct Recipient_Domain) desc\r\n",
        "    limit 100\r\n",
        ")''')\r\n",
        "\r\n",
        "df_messages.write.mode(\"overwrite\").saveAsTable(\"messagestempdata\")\r\n",
        "\r\n",
        "\r\n",
        "df_messages = spark.sql('''select * from messagestempdata where Recipient in \r\n",
        "(\r\n",
        "    select Recipient from messagestempdata\r\n",
        "    group by Recipient\r\n",
        "    order by count(distinct Sender_Domain) desc\r\n",
        "    limit 100\r\n",
        ")''')\r\n",
        "\r\n",
        "df_messages.write.mode(\"overwrite\").saveAsTable(\"messagesdata\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id, col\r\n",
        "df_events = spark.sql('''select Id, puser, Id as ICalUid, Subject, False as Recurrence, False as IsCancelled,\r\n",
        "             SentDateTime as Start, SentDate as Start_Date, SentDateTime as End, Sender as Organizer, Recipient as Attendee, \r\n",
        "             'Accepted' as Attendee_Response, 'Required' as Attendee_Type, Sender_Domain as Organizer_Domain, Recipient_Domain as Attendee_Domain,\r\n",
        "             2 as Attendees_cnt, 'Remote' as location_flag from messagesdata''')\r\n",
        "\r\n",
        "df_events = df_events.withColumn(\"idx\", monotonically_increasing_id())\r\n",
        "df_events = df_events.filter((col('idx') % 2 == 0) | (col('idx') % 3 == 0) )\r\n",
        "\r\n",
        "df_events = df_events.drop('idx')\r\n",
        "df_events.write.mode(\"overwrite\").saveAsTable(\"eventsdata\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_domains = spark.sql('select distinct Sender_Domain from messagesdata')\r\n",
        "df_source_domains.write.mode(\"overwrite\").saveAsTable(\"sender_domains\")\r\n",
        "\r\n",
        "df_target_domains = spark.sql('select distinct Recipient_Domain from messagesdata')\r\n",
        "df_target_domains.write.mode(\"overwrite\").saveAsTable(\"recipient_domains\")\r\n",
        "\r\n",
        "\r\n",
        "sql_str = '''select distinct domain from (\r\n",
        "    select distinct Sender_Domain as domain from messagesdata \r\n",
        "    union all \r\n",
        "    select distinct Recipient_Domain as domain from messagesdata \r\n",
        ")'''\r\n",
        "df_all_domains = spark.sql(sql_str)\r\n",
        "df_all_domains.write.mode(\"overwrite\").saveAsTable(\"all_domains\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql_str = '''\r\n",
        "select distinct employee_name,employee_email,employee_domain\r\n",
        "from\r\n",
        "(\r\n",
        "    select distinct replace(split(sender, '@')[0],'.', ' ') as employee_name, Sender as employee_email, Sender_domain as employee_domain\r\n",
        "    from messagesdata where Sender_Domain = \\'enron.com\\'\r\n",
        "\r\n",
        "    union all \r\n",
        "\r\n",
        "    select distinct replace(split(Recipient, '@')[0],'.', ' ') as employee_name, Recipient as employee_email, Recipient_domain as employee_domain\r\n",
        "    from messagesdata where Recipient_domain = \\'enron.com\\'\r\n",
        ")\r\n",
        "'''\r\n",
        "\r\n",
        "df_employees = spark.sql(sql_str)\r\n",
        "df_employees.write.mode(\"overwrite\").saveAsTable(\"employees\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql_str = '''\r\n",
        "select distinct contact_name,contact_email,contact_domain\r\n",
        "from\r\n",
        "(\r\n",
        "    select distinct replace(split(sender, '@')[0],'.', ' ') as contact_name, sender as contact_email, sender_domain as contact_domain\r\n",
        "    from messagesdata where sender_domain != \\'enron.com\\'\r\n",
        "\r\n",
        "    union all \r\n",
        "\r\n",
        "    select distinct replace(split(Recipient, '@')[0],'.', ' ') as contact_name, Recipient as contact_email, Recipient_domain as contact_domain\r\n",
        "    from messagesdata where Recipient_domain != \\'enron.com\\'\r\n",
        ")\r\n",
        "'''\r\n",
        "df_contacts = spark.sql(sql_str)\r\n",
        "df_contacts.write.mode(\"overwrite\").saveAsTable(\"contacts\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql_str = '''\r\n",
        "select distinct name,email,domain,contact_type\r\n",
        "from\r\n",
        "(\r\n",
        "    select distinct employee_name as name, employee_email as email, employee_domain as domain, 'employee' as contact_type\r\n",
        "    from employees\r\n",
        "    union all \r\n",
        "    select distinct contact_name as name, contact_email as email, contact_domain as domain, 'contact' as contact_type\r\n",
        "    from contacts\r\n",
        ")\r\n",
        "'''\r\n",
        "df_emp_contacts = spark.sql(sql_str)\r\n",
        "df_emp_contacts.write.mode(\"overwrite\").saveAsTable(\"emp_contacts\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_links = spark.sql('''select sender, recipient, count(*) as emailcount from messagesdata group by sender, recipient''')\r\n",
        "display(df_links.take(2))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\r\n",
        "G = nx.DiGraph()\r\n",
        "\r\n",
        "for row in df_links.collect():\r\n",
        "    G.add_edge(row.sender, row.recipient, weight= 1 / int(row.emailcount))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "import networkx as nx\r\n",
        "\r\n",
        "def get_eigen_centrality(G):\r\n",
        "    ec = nx.eigenvector_centrality(G,weight='weight')\r\n",
        "    df_ec = spark.createDataFrame(ec.items())\r\n",
        "    cols = ['Emp','EigenCentrality']\r\n",
        "    df_ec = df_ec.toDF(*cols)\r\n",
        "    df_ec = df_ec.withColumn('EigenCentrality', round((col('EigenCentrality') * lit(10)),3))\r\n",
        "    df_ec = df_ec.sort(df_ec.EigenCentrality.desc()) \r\n",
        "    return df_ec\r\n",
        "    \r\n",
        "df_ec = get_eigen_centrality(G)\r\n",
        "# display(df_ec)\r\n",
        "df_ec.write.mode(\"overwrite\").saveAsTable(\"eigen_centralities\")                 "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_degree_centrality(G):\r\n",
        "    ec = nx.degree_centrality(G)\r\n",
        "    df_ec = spark.createDataFrame(ec.items())\r\n",
        "    cols = ['Emp','DegreeCentrality']\r\n",
        "    df_ec = df_ec.toDF(*cols)\r\n",
        "    df_ec = df_ec.withColumn('DegreeCentrality', round((col('DegreeCentrality') * lit(10)),3))\r\n",
        "    df_ec = df_ec.sort(df_ec.DegreeCentrality.desc()) \r\n",
        "    return df_ec\r\n",
        "    \r\n",
        "df_dc = get_degree_centrality(G)\r\n",
        "df_dc.write.mode(\"overwrite\").saveAsTable(\"degree_centralities\")                 "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_ANALYTICS_KEY = TokenLibrary.getSecret(keyvault_name,text_analytics_key,KeyVaultLinkedServiceName)\r\n",
        "TEXT_ANALYTICS_ENDPOINT = TokenLibrary.getSecret(keyvault_name,text_analytics_endpoint,KeyVaultLinkedServiceName)\r\n",
        "TEXT_ANALYTICS_REGION = TokenLibrary.getSecret(keyvault_name,text_analytics_region,KeyVaultLinkedServiceName)\r\n",
        "\r\n",
        "df_data = spark.sql('''select sender, recipient, subject,uniquebody from enrondata limit 1000''')\r\n",
        "\r\n",
        "\r\n",
        "from azure.core.credentials import AzureKeyCredential\r\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
        "def authenticate_client():\r\n",
        "    \"\"\"\r\n",
        "    Returns: text analytics client\r\n",
        "    \"\"\"\r\n",
        "    ta_credential = AzureKeyCredential(TEXT_ANALYTICS_KEY)\r\n",
        "    text_analytics_client = TextAnalyticsClient(\r\n",
        "            endpoint=TEXT_ANALYTICS_ENDPOINT, \r\n",
        "            credential=ta_credential)\r\n",
        "    return text_analytics_client\r\n",
        "text_analytics_client = authenticate_client()\r\n",
        "\r\n",
        "def get_sentiment(inp_text):\r\n",
        "    documents = [inp_text]\r\n",
        "    response = text_analytics_client.analyze_sentiment(documents = documents)[0]  \r\n",
        "    try:\r\n",
        "        overallscore = response.confidence_scores.positive + (0.5*response.confidence_scores.neutral) \r\n",
        "        return response.sentiment, overallscore\r\n",
        "    except Exception as err:\r\n",
        "        print(\"Encountered Sentiment exception. {}\".format(err))\r\n",
        "        return \"Neutral\",0.5\r\n",
        "def get_ner(inp_text):\r\n",
        "    try:\r\n",
        "        documents = [inp_text]\r\n",
        "        result = text_analytics_client.recognize_entities(documents = documents)[0]  \r\n",
        "        return [{\"text\": x.text, \"category\": x.category, \"subcategory\": x.subcategory, \"length\": x.length, \"offset\": x.offset, \"confidence_score\": x.confidence_score} for x in result.entities]\r\n",
        "    except Exception as err:\r\n",
        "        print(\"Encountered NER exception. {}\".format(err))\r\n",
        "    return []\r\n",
        "def get_key_phrases(inp_text):\r\n",
        "    try:\r\n",
        "      documents = [inp_text]\r\n",
        "      response = text_analytics_client.extract_key_phrases(documents = documents)[0] \r\n",
        "      if not response.is_error:\r\n",
        "          return response.key_phrases\r\n",
        "      else:\r\n",
        "          print(response.id, response.error)\r\n",
        "    except Exception as err:\r\n",
        "      print(\"Encountered Translation exception. {}\".format(err))\r\n",
        "    return []\r\n",
        "\r\n",
        "from pyspark.sql import Row\r\n",
        "\r\n",
        "schema = MapType(StringType(),ArrayType(MapType(StringType(),StringType())))\r\n",
        "        \r\n",
        "def get_ners(content):\r\n",
        "    named_entities = []\r\n",
        "    named_entities = get_ner(content) \r\n",
        "    named_entity_obj = {\"en\": named_entities}\r\n",
        "    return(named_entity_obj)\r\n",
        "\r\n",
        "get_ner_udf = udf(lambda content: get_ners(content),returnType=schema)\r\n",
        "\r\n",
        "df_ner = df_data.select(['sender', 'recipient', 'subject','content']) \\\r\n",
        "                .withColumn('NER', get_ner_udf(col(\"content\"))) \\\r\n",
        "                .select(explode(col(\"NER\")).alias(\"language\",\"ner1\"),'sender','recipient') \\\r\n",
        "                .select(explode(col(\"ner1\")).alias(\"ner2\"),'sender','recipient','language') \\\r\n",
        "                .select(['sender','recipient','language',col(\"ner2.text\").alias(\"value\"),col(\"ner2.subcategory\").alias(\"subcategory\"), \\\r\n",
        "                col(\"ner2.offset\").alias(\"offset\"),col(\"ner2.confidence_score\").alias(\"confidence_score\"), col(\"ner2.category\").alias(\"category\"), \\\r\n",
        "                col(\"ner2.length\").alias(\"length\")])\r\n",
        "\r\n",
        "df_ner.write.mode(\"append\").saveAsTable(\"messages_entities\") "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Spacy Text Analytics\r\n",
        "\r\n",
        "# import subprocess\r\n",
        "# import sys\r\n",
        "# process = subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\r\n",
        "# # print(process.stdout.decode('utf-8'))\r\n",
        "\r\n",
        "# df_data = spark.sql('''select sender, recipient, subject,uniquebody from messagesdata''')\r\n",
        "# # display(df_data.take(2))\r\n",
        "\r\n",
        "# import spacy \r\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "\r\n",
        "# def get_ner(inp_text):\r\n",
        "#     try:  \r\n",
        "#         doc = nlp(inp_text)\r\n",
        "#         result = []\r\n",
        "#         for ent in doc.ents:\r\n",
        "#             if ent.label_ in (['PERSON','ORG','GPE']):\r\n",
        "#                 ent1 = {\r\n",
        "#                     \"text\": ent.text,\r\n",
        "#                     \"category\": ent.label_,\r\n",
        "#                     \"subcategory\": ent.label_,\r\n",
        "#                     \"length\": 0,\r\n",
        "#                     \"offset\": 0,\r\n",
        "#                     \"confidence_score\": 1.0\r\n",
        "#                 }\r\n",
        "#                 result.append(ent1)\r\n",
        "#         return [{\"text\": x['text'], \"category\": x['category'], \"subcategory\": x['subcategory'], \"length\": ['length'], \"offset\": x['offset'], \"confidence_score\": x['confidence_score']} for x in result]\r\n",
        "#         #return [{\"text\": x.text, \"category\": x.category, \"subcategory\": x.subcategory, \"length\": x.length, \"offset\": x.offset, \"confidence_score\": x.confidence_score} for x in result]\r\n",
        "#     except Exception as err:\r\n",
        "#         print(\"Encountered NER exception. {}\".format(err))\r\n",
        "#         return []\r\n",
        "\r\n",
        "\r\n",
        "# from pyspark.sql.types import *\r\n",
        "# from pyspark.sql.functions import *\r\n",
        "\r\n",
        "# schema = MapType(StringType(),ArrayType(MapType(StringType(),StringType())))\r\n",
        "        \r\n",
        "# def get_ners(content):\r\n",
        "#     named_entities = []\r\n",
        "#     named_entities = get_ner(content) \r\n",
        "#     named_entity_obj = {\"en\": named_entities}\r\n",
        "#     return(named_entity_obj)\r\n",
        "\r\n",
        "# get_ner_udf = udf(lambda content: get_ners(content),returnType=schema)\r\n",
        "\r\n",
        "# df_ner = df_data.select(['sender', 'recipient', 'subject','uniquebody']) \\\r\n",
        "#                 .withColumn('NER', get_ner_udf(col(\"uniquebody\"))) \\\r\n",
        "#                 .select(explode(col(\"NER\")).alias(\"language\",\"ner1\"),'sender','recipient') \\\r\n",
        "#                 .select(explode(col(\"ner1\")).alias(\"ner2\"),'sender','recipient','language') \\\r\n",
        "#                 .select(['sender','recipient','language',col(\"ner2.text\").alias(\"value\"),col(\"ner2.subcategory\").alias(\"subcategory\"), \\\r\n",
        "#                 col(\"ner2.offset\").alias(\"offset\"),col(\"ner2.confidence_score\").alias(\"confidence_score\"), col(\"ner2.category\").alias(\"category\"), \\\r\n",
        "#                 col(\"ner2.length\").alias(\"length\")])\r\n",
        "\r\n",
        "# df_ner.write.mode(\"overwrite\").saveAsTable(\"messages_entities\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}