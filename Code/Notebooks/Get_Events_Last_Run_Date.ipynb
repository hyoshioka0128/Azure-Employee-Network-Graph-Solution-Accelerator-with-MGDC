{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# convert string parameter to list of strings\n",
        "delta_load_user_group= ast.literal_eval(delta_load_user_group)\n",
        "initial_load_user_group = ast.literal_eval(initial_load_user_group)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "base_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/\"\n",
        "\n",
        "if initialLoad == 'true':\n",
        "    user_group = initial_load_user_group\n",
        "    user_group_name = initial_load_user_group_name\n",
        "else: \n",
        "    user_group = delta_load_user_group\n",
        "    user_group_name = delta_load_user_group_name\n",
        "    \n",
        "\n",
        "messagesPath = base_path +\"o365data/\" + folder_name + \"/\" + user_group_name + \"/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import msal\n",
        "import logging\n",
        "\n",
        "\n",
        "linked_service = \"\"\n",
        "keyvault_name = \"\"\n",
        "\n",
        "authority = mssparkutils.credentials.getSecret(linkedService=linked_service,akvName=keyvault_name,secret='')\n",
        "client_id = mssparkutils.credentials.getSecret(linkedService=linked_service,akvName=keyvault_name,secret='')\n",
        "secret = mssparkutils.credentials.getSecret(linkedService=linked_service,akvName=keyvault_name,secret='')\n",
        "\n",
        "\n",
        "scope = [\"https://graph.microsoft.com/.default\"]\n",
        "base_endpoint = \"https://graph.microsoft.com/v1.0\"\n",
        "\n",
        "\n",
        "app = msal.ConfidentialClientApplication(client_id, authority=authority,client_credential=secret)\n",
        "\n",
        "result = None\n",
        "result = app.acquire_token_silent(scope, account=None)\n",
        "\n",
        "if not result:\n",
        "    logging.info(\"No suitable token exists in cache. Let's get a new one from AAD.\")\n",
        "    result = app.acquire_token_for_client(scope)\n",
        "\n",
        "if \"access_token\" not in result:\n",
        "    print(result.get(\"error\"))\n",
        "    print(result.get(\"error_description\"))\n",
        "    print(result.get(\"correlation_id\"))  # You may need this when reporting a bug\n",
        "\n",
        "headers = {'Authorization': 'Bearer ' + result['access_token']}\n",
        "\n",
        "\n",
        "def get_group_members(base_endpoint, headers, group_id):\n",
        "    endpoint = base_endpoint + '/groups/' + group_id + '/members' \n",
        "    graph_data = requests.get(endpoint,headers=headers,).json()\n",
        "    return(graph_data)\n",
        "\n",
        "def delete_group_members(base_endpoint, headers, group_id,member_ids):\n",
        "    for member_id in member_ids:\n",
        "        endpoint = base_endpoint + '/groups/' + group_id + '/members/' + member_id + '/$ref'\n",
        "        response = requests.delete(endpoint, headers=headers)\n",
        "    return(response)\n",
        "\n",
        "def add_group_members(base_endpoint, headers, group_id,members_data):\n",
        "    endpoint = base_endpoint + '/groups/' + group_id\n",
        "    response = requests.patch(endpoint,json=members_data, headers=headers)\n",
        "    return(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime,timezone\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def get_date_range(initialLoad,base_endpoint,headers,user_group,messagesPath, initial_load_user_group_name): \n",
        "    from_date = '2020-01-01 00:00:01'\n",
        "\n",
        "    date_range = ''\n",
        "\n",
        "    if initialLoad == 'true':\n",
        "        #add members from config file to AD group\n",
        "        \n",
        "\n",
        "        df = spark.read.format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDB\")\\\n",
        "            .option(\"spark.cosmos.container\", \"users\")\\\n",
        "            .load()\n",
        "        \n",
        "         # Uncomment this if your Linked Service is enabled with a private endpoint \n",
        "        #df = spark.read.format(\"cosmos.oltp\")\\\n",
        "        #    .option(\"spark.cosmos.useGatewayMode\", True)\\\n",
        "        #    .option(\"spark.synapse.linkedService\", \"CosmosDB\")\\\n",
        "        #    .option(\"spark.cosmos.container\", \"users\")\\\n",
        "        #    .load()\n",
        "\n",
        "        df = df.filter(col('initialload_completion_flag') == 0)\n",
        "        member_ids = df.select(\"user_id\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "        if len(member_ids) == 0:\n",
        "            return('Success')\n",
        "        \n",
        "        # get start and end dates\n",
        "        from_date = str(datetime.strptime(from_date, \"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "    \n",
        "    else:\n",
        "        #check if there are any members in delta group\n",
        "        df_members = get_group_members(base_endpoint,headers,delta_load_user_group[0])\n",
        "        mcount = len(df_members['value'])\n",
        "        \n",
        "        # what to do if memer count is zero?\n",
        "        if mcount == 0:\n",
        "            return('Success')\n",
        "\n",
        "         # get start and end dates\n",
        "        try: \n",
        "            folders = mssparkutils.fs.ls(messagesPath)\n",
        "            if len(folders) == 0: \n",
        "            # delta load folder is empty only the first time delta load is run for each user group \n",
        "                user_group_name = initial_load_user_group_name\n",
        "                messagesPath = base_path +\"o365data/\" + folder_name + \"/\" + user_group_name + \"/\"\n",
        "        except: \n",
        "            user_group_name = initial_load_user_group_name\n",
        "            messagesPath = base_path +\"o365data/\" + folder_name + \"/\" + user_group_name + \"/\"\n",
        "        \n",
        "        folders = mssparkutils.fs.ls(messagesPath)\n",
        "        foldernames = [f.name for f in folders]\n",
        "        foldernames.sort(reverse=True)\n",
        "        from_date = foldernames[0].split('_')[1]\n",
        "        from_date = str(datetime.strptime(from_date, \"%Y-%m-%d-%H-%M-%S\"))\n",
        "        if from_date == None:\n",
        "            from_date = '2020-01-01 00:00:00'\n",
        "\n",
        "    to_date = datetime.strptime(str(datetime.now()), \"%Y-%m-%d %H:%M:%S.%f\")\n",
        "    to_date = to_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    date_range = from_date + '_' + to_date\n",
        "    return(date_range)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "date_range = get_date_range(initialLoad,base_endpoint,headers,user_group,messagesPath, initial_load_user_group_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mssparkutils.notebook.exit(date_range)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Python 3.7.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
