{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "data_lake_account_name = '' # Synapse Workspace ADLS\n",
        "file_system_name = ''\n",
        "user_group_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "base_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#process user data\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def load_users_json_file(usersPath):\n",
        "    users_df= spark.read.load(usersPath, format='json')\n",
        "    users_df = users_df.select(\"id\",\"userPrincipalName\",\"mail\",\"mailNickname\",\"puser\",\"ptenant\",\"usageLocation\")\n",
        "\n",
        "    \n",
        "    users_path = base_path + \"o365data_processed/users/\" + user_group_name \n",
        "    users_df.write.format(\"parquet\").mode(\"append\").save(users_path)\n",
        "\n",
        "def get_users_subfolder_files(folder):\n",
        "    children = mssparkutils.fs.ls(folder)\n",
        "    for child in children:\n",
        "        if child.name == 'metadata':\n",
        "            continue\n",
        "        if child.isDir:\n",
        "           get_users_subfolder_files(child.path)\n",
        "        else:       \n",
        "            load_users_json_file(child.path)\n",
        "            \n",
        "usersPath = base_path + \"o365data/users/\" + user_group_name \n",
        "get_users_subfolder_files(usersPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#process mail folder data\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def load_mailfolders_json_file(mailfodlersPath):\n",
        "    mailfolders_df= spark.read.load(mailfodlersPath, format='json')\n",
        "    mailfolders_df = mailfolders_df.select(\"id\",\"displayName\",\"parentFolderId\",\"puser\",\"ptenant\")\n",
        "\n",
        "    mailfolder_path = base_path + \"/o365data_processed/mailfolders/\" + user_group_name \n",
        "    mailfolders_df.write.format(\"parquet\").mode(\"append\").save(mailfolder_path)\n",
        "\n",
        "def get_mailfolders_subfolder_files(folder):\n",
        "    children = mssparkutils.fs.ls(folder)\n",
        "    for child in children:\n",
        "        if child.name == 'metadata':\n",
        "            continue\n",
        "        if child.isDir:\n",
        "           get_mailfolders_subfolder_files(child.path)\n",
        "        else:\n",
        "            load_mailfolders_json_file(child.path)\n",
        "            \n",
        "mailfoldersPath = base_path + \"o365data/mailfolders/\" + user_group_name\n",
        "get_mailfolders_subfolder_files(mailfoldersPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "base_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "users_path = base_path + \"o365data_processed/users/\" + user_group_name \n",
        "mailfolder_path = base_path + \"o365data_processed/mailfolders/\" + user_group_name \n",
        "\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "df_folder_config = spark.read.format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDB\")\\\n",
        "            .option(\"spark.cosmos.container\", \"users\")\\\n",
        "            .load()\n",
        "\n",
        " # Uncomment this if your Linked Service is enabled with a private endpoint \n",
        "#df_folder_config = spark.read.format(\"cosmos.oltp\")\\\n",
        "#           .option(\"spark.cosmos.useGatewayMode\", True)\\\n",
        "#            .option(\"spark.synapse.linkedService\", \"CosmosDB\")\\\n",
        "#            .option(\"spark.cosmos.container\", \"users\")\\\n",
        "#            .load()\n",
        "\n",
        "df_folder_config = df_folder_config.select('email','folders').select(explode(col(\"folders\")).alias(\"folders\"),'email') \\\n",
        "               .select([\"email\",col(\"folders.FolderName\").alias(\"FolderName\")])\n",
        "\n",
        "\n",
        "df_users = spark.read.format('parquet').load(users_path, header=True)\n",
        "df_mailfolder = spark.read.format('parquet').load(mailfolder_path, header=True)\n",
        "\n",
        "df_folder_filter = df_folder_config.join(df_users, lower(df_folder_config['email']) == lower(df_users['UserPrincipalName'])) \\\n",
        "                .join(df_mailfolder, (df_folder_config['FolderName'] == df_mailfolder['displayName']) & (df_mailfolder['puser'] == df_users['puser'])) \\\n",
        "                .select(df_users.puser,df_users.ptenant,df_mailfolder.id, df_mailfolder.parentFolderId)\n",
        "\n",
        "folderfilter_path = base_path + \"o365data_processed/folderfiltersdata/\" #+ user_group_name\n",
        "df_folder_filter.write.format(\"parquet\").mode(\"append\").save(folderfilter_path)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Python 3.7.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8"
    },
    "save_output": true,
    "vscode": {
      "interpreter": {
        "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
